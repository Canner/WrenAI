apiVersion: v1
kind: ConfigMap
metadata:
  name: wren-config
data:
  # Wren Engine Service Port
  WREN_ENGINE_PORT: "8080"
  # Wren AI Service Port
  WREN_AI_SERVICE_PORT: "5555"

  WREN_UI_ENDPOINT: http://wren-ui-svc:3000

  #Release version used by wren ui https://github.com/Canner/WrenAI/blob/main/docker/docker-compose.yaml#L85-L88
  WREN_PRODUCT_VERSION: "0.12.0"
  WREN_ENGINE_VERSION: "0.12.3"
  WREN_AI_SERVICE_VERSION: "0.12.1"
  WREN_UI_VERSION: "0.17.6"

  # Document store related
  QDRANT_HOST: "wren-qdrant"

  # Telemetry
  POSTHOG_HOST: "https://app.posthog.com"
  TELEMETRY_ENABLED: "false"
  # this is for telemetry to know the model, i think ai-service might be able to provide a endpoint to get the information
  GENERATION_MODEL: "gpt-4o-mini-2024-07-18"

  # service endpoints of AI service & engine service
  WREN_ENGINE_ENDPOINT: "http://wren-engine-svc:8080"
  WREN_AI_ENDPOINT: "http://wren-ai-service-svc:5555"
  #WREN_AI_ENDPOINT: "http://wren-ai-service-svc.ai-system.svc.cluster.local:5555"

  # "pg" for postgres as UI application database
  WREN_UI_DB_TYPE: pg

  #For bootstrap
  WREN_ENGINE_DATA_PATH: "/app/data"

  ### if DB_TYPE = "postgres" you must provide PG_URL string in the *Secret* manifest file (deployment/kustomizations/examples/secret-wren_example.yaml) to connect to postgres

  #DEBUG, INFO
  LOGGING_LEVEL: INFO

  IBIS_SERVER_ENDPOINT: http://wren-ibis-server-svc:8000
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wren-ai-service-config
data:
  config.yaml: |
    type: llm
    provider: litellm_llm
    timeout: 120
    models:
    - model: gpt-4o-mini-2024-07-18
      api_base: https://api.openai.com/v1
      api_key_name: LLM_OPENAI_API_KEY
      kwargs:
        temperature: 0
        n: 1
        seed: 0
        max_tokens: 4096
    - model: gpt-4o-2024-08-06
      api_base: https://api.openai.com/v1
      api_key_name: LLM_OPENAI_API_KEY
      kwargs:
        temperature: 0
        n: 1
        seed: 0
        max_tokens: 4096
    - model: o3-mini-2025-01-31
      api_base: https://api.openai.com/v1
      api_key_name: LLM_OPENAI_API_KEY
      kwargs:
        n: 1
        seed: 0
        max_completion_tokens: 4096
        reasoning_effort: low

    ---
    type: embedder
    provider: litellm_embedder
    models:
    - model: text-embedding-3-large
      api_base: https://api.openai.com/v1
      api_key_name: EMBEDDER_OPENAI_API_KEY
      timeout: 120

    ---
    type: engine
    provider: wren_ui
    endpoint: http://wren-ui-svc:3000

    ---
    type: document_store
    provider: qdrant
    location: http://wren-qdrant:6333
    embedding_model_dim: 3072
    timeout: 120

    ---
    type: pipeline
    pipes:
      - name: db_schema_indexing
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: historical_question_indexing
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: table_description_indexing
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: db_schema_retrieval
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: historical_question_retrieval
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: sql_generation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: sql_correction
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: followup_sql_generation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: sql_summary
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_answer
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: sql_breakdown
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: sql_expansion
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: sql_explanation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_regeneration
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: semantics_description
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: relationship_recommendation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: question_recommendation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: question_recommendation_db_schema_retrieval
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: question_recommendation_sql_generation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        engine: wren_ui
      - name: intent_classification
        llm: litellm_llm.gpt-4o-mini-2024-07-18
        embedder: litellm_embedder.text-embedding-3-large
        document_store: qdrant
      - name: data_assistance
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_pairs_indexing
        document_store: qdrant
        embedder: litellm_embedder.text-embedding-3-large
      - name: sql_pairs_deletion
        document_store: qdrant
        embedder: litellm_embedder.text-embedding-3-large 
      - name: sql_pairs_retrieval
        document_store: qdrant
        embedder: litellm_embedder.text-embedding-3-large
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: preprocess_sql_data
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_executor
        engine: wren_ui
      - name: chart_generation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: chart_adjustment
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_question_generation
        llm: litellm_llm.gpt-4o-mini-2024-07-18
      - name: sql_generation_reasoning
        llm: litellm_llm.gpt-4o-mini-2024-07-18

    ---
    settings:
      column_indexing_batch_size: 50
      table_retrieval_size: 10
      table_column_retrieval_size: 100
      allow_using_db_schemas_without_pruning: false
      query_cache_maxsize: 1000
      query_cache_ttl: 3600
      langfuse_host: https://cloud.langfuse.com
      langfuse_enable: true
      logging_level: DEBUG
      development: false