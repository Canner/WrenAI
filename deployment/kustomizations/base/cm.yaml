apiVersion: v1
kind: ConfigMap
metadata:
  name: wren-config
data:
  # Wren Engine Service Port
  WREN_ENGINE_PORT: "8080"
  # Wren AI Service Port
  WREN_AI_SERVICE_PORT: "5555"

  WREN_UI_ENDPOINT: http://wren-ui-svc:3000

  #Release version used by wren ui https://github.com/Canner/WrenAI/blob/main/docker/docker-compose.yaml#L85-L88
  WREN_PRODUCT_VERSION: "0.9.1"
  WREN_ENGINE_VERSION: "0.11.1"
  WREN_AI_SERVICE_VERSION: "0.9.0"
  WREN_UI_VERSION: "0.13.4"

  # Document store related
  QDRANT_HOST: "wren-qdrant"

  # Telemetry
  POSTHOG_HOST: "https://app.posthog.com"
  TELEMETRY_ENABLED: "false"
  # this is for telemetry to know the model, i think ai-service might be able to provide a endpoint to get the information
  GENERATION_MODEL: "gpt-4o-mini"

  # service endpoints of AI service & engine service
  WREN_ENGINE_ENDPOINT: "http://wren-engine-svc:8080"
  WREN_AI_ENDPOINT: "http://wren-ai-service-svc:5555"
  #WREN_AI_ENDPOINT: "http://wren-ai-service-svc.ai-system.svc.cluster.local:5555"

  # "pg" for postgres as UI application database
  WREN_UI_DB_TYPE: pg

  #For bootstrap
  WREN_ENGINE_DATA_PATH: "/app/data"

  ### if DB_TYPE = "postgres" you must provide PG_URL string in the *Secret* manifest file (deployment/kustomizations/examples/secret-wren_example.yaml) to connect to postgres

  #DEBUG, INFO
  LOGGING_LEVEL: INFO

  IBIS_SERVER_ENDPOINT: http://wren-ibis-server-svc:8000
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: wren-ai-service-config
data:
  config.yaml: |
    type: llm
    provider: openai_llm
    models:
      - model: gpt-4o-mini
        kwargs:
          {
            "temperature": 0,
            "n": 1,
            "max_tokens": 4096,
            "response_format": { "type": "json_object" },
          }
    api_base: https://api.openai.com/v1

    ---
    type: embedder
    provider: openai_embedder
    models:
      - model: text-embedding-3-large
        dimension: 3072
    api_base: https://api.openai.com/v1
    timeout: 120

    ---
    type: engine
    provider: wren_ui
    endpoint: http://wren-ui-svc:3000

    ---
    type: document_store
    provider: qdrant
    location: http://wren-qdrant:6333
    embedding_model_dim: 3072
    timeout: 120

    ---
    type: pipeline
    pipes:
      - name: indexing
        embedder: openai_embedder.text-embedding-3-large
        document_store: qdrant
      - name: retrieval
        llm: openai_llm.gpt-4o-mini
        embedder: openai_embedder.text-embedding-3-large
        document_store: qdrant
      - name: historical_question
        embedder: openai_embedder.text-embedding-3-large
        document_store: qdrant
      - name: sql_generation
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: sql_correction
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: followup_sql_generation
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: sql_summary
        llm: openai_llm.gpt-4o-mini
      - name: sql_answer
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: sql_breakdown
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: sql_expansion
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: sql_explanation
        llm: openai_llm.gpt-4o-mini
      - name: sql_regeneration
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui
      - name: semantics_description
        llm: openai_llm.gpt-4o-mini
      - name: relationship_recommendation
        llm: openai_llm.gpt-4o-mini
        engine: wren_ui

    ---
    settings:
      column_indexing_batch_size: 50
      table_retrieval_size: 10
      table_column_retrieval_size: 1000
      query_cache_maxsize: 1000
      query_cache_ttl: 3600
      langfuse_host: https://cloud.langfuse.com
      langfuse_enable: false
      enable_timer: false
      development: false
