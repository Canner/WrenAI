# WrenAI Example Configuration for Ollama (via LiteLLM)
#
# HOW TO USE:
# 1. RENAME this file to 'config.yaml' and place it in ~/.wrenai/
# 2. CONFIGURE your ~/.wrenai/.env file:
#    ##HOST_PORT=3000 (this host is for ollama in env) <--- Add this line with your Ollama port
#    # Also set 'WREN_AI_SERVICE_VERSION=<your_wrenai_version>' (e.g., WREN_AI_SERVICE_VERSION=0.19.3)
# 3. ADJUST the settings below, especially:
#    - The 'api_base' URLs in the LLM and Embedder sections. Use the format:
#      #api_base: http://localhost/:<ollama_port>  <--- Use the port set in HOST_PORT
#    - The 'model' names if you are using different Ollama models.
#    - Verify the 'pipes' section against the example for your SPECIFIC WrenAI version if you encounter issues.

type: llm
provider: litellm_llm
models:
  # --- IMPORTANT ---
  # Use the format http://localhost:<ollama_port> corresponding to HOST_PORT in .env
  - api_base: http://localhost:5000 # ADJUST THIS URL (Example using port 5000)
    model: ollama_chat/gemma2:2b     # Example: Use 'ollama_chat/<your_llm_model_name>'
    alias: default                   # Default LLM alias
    timeout: 600                     # Increase timeout for potentially slower local models
    kwargs:
      n: 1
      temperature: 0

---
type: embedder
provider: litellm_embedder
models:
  # --- IMPORTANT ---
  # Use the format http://localhost:<ollama_port> corresponding to HOST_PORT in .env
  - api_base: http://localhost:5000 # ADJUST THIS URL (Example using port 5000)
    model: ollama/nomic-embed-text   # Example: Use 'ollama/<your_embedding_model_name>'
    alias: default                   # Default embedder alias
    timeout: 600                     # Increase timeout for potentially slower local models

---
# Standard Engine Endpoint (Usually default in Docker)
type: engine
provider: wren_ui
endpoint: http://wren-ui:3000

---
# Document Store (Qdrant)
type: document_store
provider: qdrant
location: http://qdrant:6333
# Embedding dimension for nomic-embed-text is 768. Adjust if using a different model.
embedding_model_dim: 768
timeout: 120
recreate_index: true # Set to false after first run to preserve embeddings

---
# Pipeline Definitions
# Uses aliases defined above (e.g., litellm_llm.default).
# Verify these pipe definitions against the official example for your WrenAI version if needed:
# Replace <WRENAI_VERSION_NUMBER> with your version (e.g., v0.19.3)
# https://raw.githubusercontent.com/canner/WrenAI/<WRENAI_VERSION_NUMBER>/docker/config.example.yaml
type: pipeline
pipes:
  - name: db_schema_indexing
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: historical_question_indexing
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: table_description_indexing
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: db_schema_retrieval
    llm: litellm_llm.default
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: historical_question_retrieval
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: sql_generation
    llm: litellm_llm.default
    engine: wren_ui
  - name: sql_correction
    llm: litellm_llm.default
    engine: wren_ui
  - name: followup_sql_generation
    llm: litellm_llm.default
    engine: wren_ui
  - name: sql_summary
    llm: litellm_llm.default
  - name: sql_answer
    llm: litellm_llm.default
    engine: wren_ui
  - name: sql_breakdown
    llm: litellm_llm.default
    engine: wren_ui
  - name: sql_expansion
    llm: litellm_llm.default
    engine: wren_ui
  - name: semantics_description
    llm: litellm_llm.default
  - name: relationship_recommendation
    llm: litellm_llm.default
    engine: wren_ui
  - name: question_recommendation
    llm: litellm_llm.default
  - name: question_recommendation_db_schema_retrieval
    llm: litellm_llm.default
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: question_recommendation_sql_generation
    llm: litellm_llm.default
    engine: wren_ui
  - name: chart_generation
    llm: litellm_llm.default
  - name: chart_adjustment
    llm: litellm_llm.default
  - name: intent_classification
    llm: litellm_llm.default
    embedder: litellm_embedder.default
    document_store: qdrant
  - name: data_assistance
    llm: litellm_llm.default
  - name: sql_pairs_indexing
    document_store: qdrant
    embedder: litellm_embedder.default
  - name: sql_pairs_retrieval
    document_store: qdrant
    embedder: litellm_embedder.default
    llm: litellm_llm.default
  - name: preprocess_sql_data
    llm: litellm_llm.default
  - name: sql_executor
    engine: wren_ui
  - name: sql_question_generation
    llm: litellm_llm.default
  - name: sql_generation_reasoning
    llm: litellm_llm.default
  # (Include other pipes as needed based on version)

---
# General Settings
settings:
  engine_timeout: 60
  column_indexing_batch_size: 50
  table_retrieval_size: 10
  table_column_retrieval_size: 100
  allow_using_db_schemas_without_pruning: true
  allow_intent_classification: true
  allow_sql_generation_reasoning: true
  query_cache_maxsize: 1000
  query_cache_ttl: 3600
  langfuse_host: https://cloud.langfuse.com
  langfuse_enable: true
  logging_level: DEBUG
  development: true
  historical_question_retrieval_similarity_threshold: 0.9
  sql_pairs_similarity_threshold: 0.7
  sql_pairs_retrieval_max_size: 10
  instructions_similarity_threshold: 0.7
  instructions_top_k: 10