## LLM
LLM_PROVIDER=openai # openai, azure-openai, ollama

# openai or openai-api-compatible llm
OPENAI_API_KEY=UTdJ2Wpm2SNx05GN2HP4TmTPiNkwFIbLs33brmXClglTOBnU
OPENAI_API_BASE=https://api.fireworks.ai/inference/v1

# azure-openai
AZURE_CHAT_BASE=
AZURE_CHAT_KEY=
AZURE_CHAT_VERSION=

AZURE_EMBED_BASE=
AZURE_EMBED_KEY=
AZURE_EMBED_VERSION=

# ollama
OLLAMA_URL=http://host.docker.internal:11434

GENERATION_MODEL=accounts/fireworks/models/llama-v3-70b-instruct
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
EMBEDDING_MODEL_DIMENSION=768


## DOCUMENT_STORE
DOCUMENT_STORE_PROVIDER=qdrant

QDRANT_HOST=qdrant